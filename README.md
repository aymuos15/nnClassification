# PyTorch Image Classifier

[![Documentation Status](https://readthedocs.org/projects/nnclassification/badge/?version=latest)](https://nnclassification.readthedocs.io/en/latest/)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

Production-ready image classification framework with flexible architecture support and index-based cross-validation. Supports training, resumption, and comprehensive evaluation.

## Quick Start

### Install
```bash
# Install in development mode (editable)
uv pip install -e .

# Or with development dependencies
uv pip install -e ".[dev]"
```

After installation, you can use the CLI commands from anywhere:
- `ml-init-config` - Generate dataset-specific configuration
- `ml-split` - Create dataset splits
- `ml-lr-finder` - Find optimal learning rate
- `ml-train` - Train models
- `ml-inference` - Run inference
- `ml-export` - Export models to ONNX format
- `ml-visualise` - Visualize data and predictions
- `ml-search` - Hyperparameter optimization (optional)

### Data Structure (MANDATORY)

```
data/your_dataset/
â”œâ”€â”€ raw/              # Your original images organized by class
â”‚   â”œâ”€â”€ class1/
â”‚   â”‚   â”œâ”€â”€ img1.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ class2/
â”‚       â”œâ”€â”€ img2.jpg
â”‚       â””â”€â”€ ...
â””â”€â”€ splits/           # Generated by splitting.py (index files, no data duplication)
    â”œâ”€â”€ test.txt          # Single test set (SAME for all folds)
    â”œâ”€â”€ fold_0_train.txt
    â”œâ”€â”€ fold_0_val.txt
    â”œâ”€â”€ fold_1_train.txt
    â”œâ”€â”€ fold_1_val.txt
    â””â”€â”€ ...
```

### Prepare Data & Train
```bash
# Step 1: Generate cross-validation splits (one time per dataset)
ml-split --raw_data data/my_dataset/raw --folds 5
# Output automatically saved to: data/my_dataset/splits/

# Step 2: Initialize configuration for your dataset (auto-detects settings)
ml-init-config data/my_dataset
# Creates: configs/my_dataset_config.yaml

# Non-interactive mode with defaults
ml-init-config data/my_dataset --yes

# Custom settings
ml-init-config data/my_dataset \
  --architecture efficientnet_b0 \
  --batch_size 32 \
  --num_epochs 50 \
  --lr 0.001

# Step 3: (Optional) Edit the generated config
vim configs/my_dataset_config.yaml

# Step 4: (Optional) Find optimal learning rate before training
ml-lr-finder --config configs/my_dataset_config.yaml
# Check runs/lr_finder_TIMESTAMP/lr_plot.png for suggested LR

# Step 5: Train on fold 0 (automatically tests on held-out test set after training)
ml-train --config configs/my_dataset_config.yaml

# Custom hyperparameters via CLI (overrides config)
ml-train --config configs/my_dataset_config.yaml --batch_size 32 --lr 0.01

# Train other folds (cross-validation)
# Note: Test set is SAME for all folds, only train/val splits change
ml-train --config configs/my_dataset_config.yaml --fold 1
ml-train --config configs/my_dataset_config.yaml --fold 2

# Resume training
ml-train --config configs/my_dataset_config.yaml --resume runs/my_dataset_fold_0/weights/last.pt
```

**Note:** After training completes, the model is automatically evaluated on the test set. Test results are saved to `runs/{run_name}/logs/classification_report_test.txt` and logged to TensorBoard.

### Inference & Export
```bash
# Standard inference
ml-inference --checkpoint_path runs/my_dataset_base_fold_0/weights/best.pt

# Test-Time Augmentation (TTA) for +1-3% accuracy
ml-inference --checkpoint_path runs/my_dataset_base_fold_0/weights/best.pt --tta

# Ensemble multiple models for +2-5% accuracy
ml-inference --ensemble \
  runs/my_dataset_fold_0/weights/best.pt \
  runs/my_dataset_fold_1/weights/best.pt \
  runs/my_dataset_fold_2/weights/best.pt

# TTA + Ensemble for maximum accuracy (+3-8%)
ml-inference --ensemble \
  runs/my_dataset_fold_0/weights/best.pt \
  runs/my_dataset_fold_1/weights/best.pt \
  --tta

# Export model to ONNX format for deployment
ml-export --checkpoint runs/my_dataset_base_fold_0/weights/best.pt
# Creates: runs/my_dataset_base_fold_0/weights/best.onnx

# Export with validation
ml-export --checkpoint runs/my_dataset_base_fold_0/weights/best.pt --validate

# Export with comprehensive validation and benchmarking
ml-export --checkpoint runs/my_dataset_base_fold_0/weights/best.pt --comprehensive-validate --benchmark
```

### Visualization

```bash
# Launch TensorBoard
ml-visualise --mode launch --run_dir runs/base

# Visualize dataset samples
ml-visualise --mode samples --run_dir runs/base --split train --num_images 16

# Visualize model predictions (green=correct, red=incorrect)
ml-visualise --mode predictions --run_dir runs/base --split val

# Clean TensorBoard logs
ml-visualise --mode clean
```

Or use TensorBoard directly:
```bash
tensorboard --logdir runs/
```

### Dataset Statistics (Optional but Recommended)

```python
# Analyze dataset before training
from ml_src.core.data import analyze_dataset, generate_statistics_report, generate_all_plots

stats = analyze_dataset('data/my_dataset/raw')
generate_statistics_report(stats, 'data/my_dataset/splits/statistics.txt')
generate_all_plots(stats, 'data/my_dataset/splits/statistics/')

# Check for issues
if stats['imbalance_ratio'] > 3.0:
    print("âš ï¸ Dataset is imbalanced - consider focal loss or class weights")
```

### Model EMA (Exponential Moving Average)

Enable EMA for **0.5-2% accuracy improvement** with zero training cost:

```yaml
# In your config file
training:
  ema:
    enabled: true
    decay: 0.9999      # 0.999-0.9999 typical
    warmup_steps: 2000  # Optional
```

EMA maintains a shadow copy of model weights for better generalization. Both regular and EMA validation metrics are logged to TensorBoard (`Accuracy/val` and `Accuracy/val_ema`).

## Configuration

Generate dataset-specific configs with `ml-init-config`, then use CLI overrides:
```bash
# Create config (auto-detects dataset info)
ml-init-config data/my_dataset

# Edit if needed
vim configs/my_dataset_config.yaml

# Train with config (required)
ml-train --config configs/my_dataset_config.yaml
```

**CLI overrides** (override config file settings):
- `--config`: **Required** - Path to config file
- `--dataset_name`: Dataset name (used in run directory naming)
- `--data_dir`: Dataset path (must contain raw/ and splits/)
- `--fold`: Which CV fold to use (0-indexed, default: 0)
- `--batch_size`: Batch size
- `--num_epochs`: Training epochs
- `--lr`: Learning rate
- `--num_workers`: Data loading workers

**Full config documentation:** See `docs/configuration/README.md`

## Output Structure
```
runs/{dataset_name}_{params}_fold_{N}/  # Auto-named based on dataset and parameters
â”œâ”€â”€ config.yaml                          # Saved configuration
â”œâ”€â”€ summary.txt                          # Training summary
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ best.pt                         # Best model (highest val accuracy)
â”‚   â””â”€â”€ last.pt                         # Latest checkpoint (for resuming)
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ train.log                       # Detailed training log
â”‚   â”œâ”€â”€ classification_report_train.txt
â”‚   â”œâ”€â”€ classification_report_val.txt
â”‚   â””â”€â”€ classification_report_test.txt  # Auto-generated test results
â””â”€â”€ tensorboard/                         # TensorBoard logs
    â””â”€â”€ events.out.tfevents.*           # Training/test metrics, plots, confusion matrices
```

**Example run directories:**
- `runs/hymenoptera_base_fold_0/` - Default training on fold 0
- `runs/hymenoptera_batch_32_fold_1/` - Batch size 32, fold 1
- `runs/custom_dataset_lr_0.01_epochs_50_fold_2/` - Custom params, fold 2

## Documentation

ğŸ“š **[Complete Documentation](docs/README.md)** - Comprehensive guides organized by topic

**Quick Links:**
- [Quick Start Guide](docs/getting-started/quick-start.md) - Train in 5 minutes
- [Data Preparation](docs/getting-started/data-preparation.md) - Organize your dataset
- [Configuration Reference](docs/configuration/README.md) - All settings explained
- [Training Guide](docs/user-guides/training.md) - Complete workflows
- [Monitoring & Visualization](docs/user-guides/monitoring.md) - TensorBoard and visualise.py
- [Troubleshooting](docs/reference/troubleshooting.md) - Common issues

**Documentation Sections:**
- **Getting Started** - Installation, data prep, quick start
- **Configuration** - Complete parameter reference
- **User Guides** - Training, inference, monitoring, tuning
- **Architecture** - System design and components
- **Development** - Extend and customize
- **Reference** - Best practices, troubleshooting, FAQ

## Requirements

- Python 3.8+
- PyTorch 2.0+
- TensorBoard 2.14+
- CUDA (optional, for GPU training)

See `pyproject.toml` for full dependencies.
