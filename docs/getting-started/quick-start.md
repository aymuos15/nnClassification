# Quick Start Guide

Get up and running with your first training run in 5 minutes!

---

## Prerequisites

- ✅ Python 3.8+ installed
- ✅ Package installed (`uv pip install -e .`)
- ✅ CLI commands available globally
- ✅ Dataset organized ([see Data Preparation](data-preparation.md))

---

## 5-Minute Quick Start

### Step 1: Verify Installation

```bash
# Check PyTorch is installed
python -c "import torch; print(f'PyTorch {torch.__version__} ready!')"

# Check CUDA (optional, can train on CPU)
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### Step 2: Use Example Dataset

The repo includes a sample dataset (ants vs bees) with splits already generated:

```bash
# Check it exists
ls data/hymenoptera_data/

# Should show: raw/ splits/
```

### Step 3: Run Training

```bash
# Train for 3 epochs (quick test) on fold 0
ml-train --fold 0 --num_epochs 3
```

**Expected output:**
```
2025-10-05 01:25:02 | INFO     | Using fold: 0
2025-10-05 01:25:02 | INFO     | Classes: ['ants', 'bees']
2025-10-05 01:25:02 | INFO     | Dataset sizes: {'train': 265, 'val': 56, 'test': 59}
2025-10-05 01:25:02 | INFO     | Starting Training
2025-10-05 01:25:02 | INFO     | Epoch 0/2
--------------------------------------------------
2025-10-05 01:25:04 | INFO     | train Loss: 0.7019 Acc: 0.5623
2025-10-05 01:25:04 | INFO     | val Loss: 0.6481 Acc: 0.6607
2025-10-05 01:25:04 | SUCCESS  | New best model saved! Acc: 0.6607
...
2025-10-05 01:25:09 | SUCCESS  | Training Complete!
```

### Step 4: Check Results

```bash
# View training outputs
ls runs/hymenoptera_base_fold_0/

# Should show:
# - config.yaml
# - summary.txt
# - weights/best.pt
# - weights/last.pt
# - logs/
# - tensorboard/
```

### Step 5: View Metrics

```bash
# Start TensorBoard
tensorboard --logdir runs/

# Open in browser: http://localhost:6006
```

**🎉 Congratulations! You've completed your first training run.**

---

## Train on Your Own Data

### Step 1: Organize Your Dataset

**Required structure:**
```
data/my_dataset/
├── raw/              # Your images organized by class
│   ├── class1/
│   │   ├── img1.jpg
│   │   └── ...
│   ├── class2/
│   │   └── ...
│   └── class3/
│       └── ...
└── splits/           # Will be generated by splitting.py
```

**See:** [Data Preparation Guide](data-preparation.md) for detailed instructions.

### Step 2: Generate Cross-Validation Splits

```bash
ml-split \
  --raw_data data/my_dataset/raw \
  --output data/my_dataset/splits \
  --folds 5 \
  --ratio 0.7 0.15 0.15 \
  --seed 42
```

**Output:**
```
2025-10-05 01:30:00 | INFO     | Generating fold 0/4...
2025-10-05 01:30:01 | SUCCESS  | Fold 0 complete: 700 train, 150 val, 150 test
2025-10-05 01:30:01 | INFO     | Generating fold 1/4...
...
2025-10-05 01:30:05 | SUCCESS  | All 5 folds generated successfully!
```

### Step 3: Generate Configuration

```bash
# Auto-generate config for your dataset
ml-init-config data/my_dataset

# This creates: configs/my_dataset_config.yaml
```

### Step 4: (Optional) Find Optimal Learning Rate

Before training, you can find the optimal learning rate using LR range test:

```bash
# Run LR finder (basic)
ml-lr-finder --config configs/my_dataset_config.yaml

# Custom LR range and iterations
ml-lr-finder --config configs/my_dataset_config.yaml --start_lr 1e-7 --end_lr 1 --num_iter 200

# Adjust early stopping sensitivity (stops when loss > threshold × min_loss)
ml-lr-finder --config configs/my_dataset_config.yaml --diverge_threshold 2.0  # More sensitive
ml-lr-finder --config configs/my_dataset_config.yaml --diverge_threshold 6.0  # Less sensitive

# Check the output plot
# runs/lr_finder_TIMESTAMP/lr_plot.png shows suggested LR
```

The LR finder will:
- Test learning rates from 1e-8 to 10.0 over 100 iterations
- Generate a plot showing loss vs learning rate
- Suggest an optimal learning rate (typically at steepest descent)
- Stop early if loss diverges (controlled by `diverge_threshold`, default: 4.0)
- Save results to `runs/lr_finder_TIMESTAMP/`

You can then update your config with the suggested learning rate before training.

### Step 5: Train

```bash
# Train fold 0
ml-train --config configs/my_dataset_config.yaml --fold 0

# Train other folds (for cross-validation)
ml-train --config configs/my_dataset_config.yaml --fold 1
ml-train --config configs/my_dataset_config.yaml --fold 2
```

### Step 6: (Optional) Export for Deployment

After training, export your best model to ONNX format for deployment:

```bash
# Export model to ONNX
ml-export --checkpoint runs/my_dataset_base_fold_0/weights/best.pt

# Export with validation to ensure correctness
ml-export --checkpoint runs/my_dataset_base_fold_0/weights/best.pt --validate

# Export with comprehensive validation and benchmarking
ml-export --checkpoint runs/my_dataset_base_fold_0/weights/best.pt \
  --validate --benchmark
```

The exported ONNX model will be saved next to the checkpoint:
- `runs/my_dataset_base_fold_0/weights/best.onnx`

ONNX models can be deployed to:
- ONNX Runtime (CPU/GPU)
- TensorRT (NVIDIA GPUs)
- OpenVINO (Intel hardware)
- Mobile devices (iOS/Android)
- Web browsers (ONNX.js)

---

## Basic Training Options

### Adjust Training Duration

```bash
# Quick test (5 epochs)
ml-train --fold 0 --num_epochs 5

# Full training (50 epochs)
ml-train --fold 0 --num_epochs 50
```

### Change Batch Size

```bash
# Smaller batch (if GPU memory limited)
ml-train --fold 0 --batch_size 16

# Larger batch (if you have powerful GPU)
ml-train --fold 0 --batch_size 64
```

### Adjust Learning Rate

```bash
# Lower learning rate (more stable)
ml-train --fold 0 --lr 0.0001

# Higher learning rate (faster convergence)
ml-train --fold 0 --lr 0.01
```

### Combine Options

```bash
ml-train --fold 0 --batch_size 32 --lr 0.01 --num_epochs 50
```

### Change Dataset

```bash
# Train on different dataset
ml-train --dataset_name custom --data_dir data/custom_dataset --fold 0
```

---

## Resume Training

If training was interrupted:

```bash
# Resume from last checkpoint
ml-train --resume runs/hymenoptera_base_fold_0/last.pt

# Resume and train more epochs
ml-train --resume runs/hymenoptera_base_fold_0/last.pt --num_epochs 50
```

---

## Run Inference

After training, evaluate on test data:

```bash
# Use best model
ml-inference --checkpoint_path runs/hymenoptera_base_fold_0/weights/best.pt
```

**Expected output:**
```
2025-10-05 01:35:00 | INFO     | Using fold: 0
2025-10-05 01:35:00 | INFO     | Test dataset size: 59
2025-10-05 01:35:00 | INFO     | Running Inference
2025-10-05 01:35:01 | SUCCESS  | Inference Complete!
2025-10-05 01:35:01 | INFO     | Test Accuracy: 0.9153

┏━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━┓
┃ Sample # ┃ True Label   ┃ Predicted    ┃ Correct   ┃
┡━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━┩
│ 1        │ ants         │ ants         │ ✓         │
│ 2        │ ants         │ ants         │ ✓         │
│ 3        │ bees         │ bees         │ ✓         │
...
```

---

## Monitor Training

### TensorBoard (Recommended)

```bash
# Start TensorBoard
tensorboard --logdir runs/

# Open browser: http://localhost:6006
```

**You can view:**
- Training and validation loss curves
- Accuracy curves
- Learning rate schedule
- Confusion matrices
- Classification reports

### Log Files

```bash
# View training log
cat runs/hymenoptera_base_fold_0/logs/train.log

# View training summary
cat runs/hymenoptera_base_fold_0/summary.txt
```

### Real-Time Monitoring

```bash
# Watch training progress
tail -f runs/hymenoptera_base_fold_0/logs/train.log

# Monitor GPU
watch -n 1 nvidia-smi
```

---

## Common Commands Cheat Sheet

```bash
# === DATA PREPARATION ===
# Generate CV splits (one time per dataset)
ml-split --raw_data data/my_dataset/raw --output data/my_dataset/splits --folds 5

# Generate config
ml-init-config data/my_dataset

# === LEARNING RATE FINDER (OPTIONAL) ===
ml-lr-finder --config configs/my_dataset_config.yaml  # Find optimal LR
ml-lr-finder --config configs/my_dataset_config.yaml --diverge_threshold 2.0  # More sensitive stopping

# === TRAINING ===
ml-train --config configs/my_dataset_config.yaml --fold 0    # Basic training
ml-train --config configs/my_dataset_config.yaml --fold 0 --num_epochs 50   # More epochs
ml-train --config configs/my_dataset_config.yaml --fold 0 --batch_size 32   # Different batch size
ml-train --config configs/my_dataset_config.yaml --fold 0 --lr 0.01         # Different learning rate
ml-train --resume runs/hymenoptera_base_fold_0/last.pt  # Resume training

# Train all folds (cross-validation)
for fold in {0..4}; do
  ml-train --config configs/my_dataset_config.yaml --fold $fold
done

# === INFERENCE ===
ml-inference --checkpoint_path runs/hymenoptera_base_fold_0/weights/best.pt

# === EXPORT (DEPLOYMENT) ===
ml-export --checkpoint runs/hymenoptera_base_fold_0/weights/best.pt  # Export to ONNX
ml-export --checkpoint runs/hymenoptera_base_fold_0/weights/best.pt --validate  # With validation

# === MONITORING ===
tensorboard --logdir runs/                              # View all runs
tail -f runs/hymenoptera_base_fold_0/logs/train.log    # Watch training log
cat runs/hymenoptera_base_fold_0/summary.txt           # View summary

# === SYSTEM ===
nvidia-smi                                              # Check GPU
watch -n 1 nvidia-smi                                   # Monitor GPU continuously
ls runs/                                                # List training runs
```

---

## Training Workflow

### Typical Workflow

1. **Prepare dataset** (one-time setup):
   ```bash
   ml-split --raw_data data/my_dataset/raw --folds 5
   ml-init-config data/my_dataset
   ```

2. **Find optimal learning rate** (optional but recommended):
   ```bash
   ml-lr-finder --config configs/my_dataset_config.yaml
   # Check runs/lr_finder_TIMESTAMP/lr_plot.png for suggested LR
   # Update config with suggested LR
   ```

3. **Quick test** (verify everything works):
   ```bash
   ml-train --config configs/my_dataset_config.yaml --fold 0 --num_epochs 3
   ```

4. **Check results** in TensorBoard:
   ```bash
   tensorboard --logdir runs/
   ```

5. **Tune hyperparameters** (try different values on fold 0):
   ```bash
   ml-train --config configs/my_dataset_config.yaml --fold 0 --lr 0.001 --batch_size 32 --num_epochs 20
   ml-train --config configs/my_dataset_config.yaml --fold 0 --lr 0.01 --batch_size 32 --num_epochs 20
   ml-train --config configs/my_dataset_config.yaml --fold 0 --lr 0.01 --batch_size 64 --num_epochs 20
   ```

6. **Full training** (with best hyperparams on all folds):
   ```bash
   for fold in {0..4}; do
     ml-train --config configs/my_dataset_config.yaml --fold $fold --lr 0.01 --batch_size 64 --num_epochs 100
   done
   ```

7. **Export best models** for deployment:
   ```bash
   for fold in {0..4}; do
     ml-export --checkpoint runs/my_dataset_lr_0.01_batch_64_epochs_100_fold_$fold/weights/best.pt --validate
   done
   ```

---

## Understanding Output Structure

After training, you'll see:

```
runs/
└── hymenoptera_base_fold_0/     # Auto-named: dataset_name + parameters + fold
    ├── config.yaml              # Saved configuration
    ├── summary.txt              # Training summary
    ├── weights/
    │   ├── best.pt             # Best model (highest val accuracy)
    │   └── last.pt             # Latest checkpoint (for resuming)
    ├── logs/
    │   ├── train.log           # Detailed training log
    │   ├── classification_report_train.txt
    │   └── classification_report_val.txt
    └── tensorboard/            # TensorBoard logs
        └── events.out.tfevents.*
```

### Key Files

- **`best.pt`** - Use for deployment/inference (highest validation accuracy)
- **`last.pt`** - Use to resume interrupted training
- **`summary.txt`** - Quick overview of training results
- **`train.log`** - Detailed training information
- **`config.yaml`** - Exact configuration used (for reproducibility)

---

## Troubleshooting Quick Fixes

### Out of Memory Error

```bash
# Reduce batch size
ml-train --fold 0 --batch_size 8

# Or train on CPU
ml-train --fold 0 --device cpu --batch_size 4
```

### Training Too Slow

```bash
# Increase workers (if CPU has many cores)
ml-train --fold 0 --num_workers 8

# Use larger batch size (if GPU has memory)
ml-train --fold 0 --batch_size 64
```

### CUDA Not Available

```bash
# Check CUDA
python -c "import torch; print(torch.cuda.is_available())"

# If False, train on CPU
ml-train --fold 0 --device cpu
```

### Index File Not Found

```
FileNotFoundError: Index file not found: data/my_dataset/splits/fold_0_train.txt
```

**Solution:** Generate splits first:
```bash
ml-split --raw_data data/my_dataset/raw --output data/my_dataset/splits --folds 5
```

### Data Loading Error

```bash
# Verify dataset structure
ls -R data/my_dataset/

# Should show raw/ with class subfolders and splits/ with index files
# See: Data Preparation Guide
```

---

## Next Steps

### Learn More

- **Data Preparation:** [Data Preparation Guide](data-preparation.md)
- **Training Guide:** [Training Workflows](../user-guides/training.md)
- **Configuration:** [Configuration Overview](../configuration/README.md)
- **Hyperparameter Tuning:** [Tuning Guide](../user-guides/hyperparameter-tuning.md)
- **Model Selection:** [Model Configuration](../configuration/models.md)

### Try Different Models

Edit config (generated by `ml-init-config`):
```yaml
model:
  type: 'base'
  architecture: 'efficientnet_b0'  # Instead of resnet18
  weights: 'DEFAULT'                # Use pretrained weights
```

Then train:
```bash
ml-train --fold 0
```

### Experiment with Hyperparameters

```bash
# Systematic search on fold 0
ml-train --fold 0 --lr 0.001 --batch_size 16 --num_epochs 20
ml-train --fold 0 --lr 0.001 --batch_size 32 --num_epochs 20
ml-train --fold 0 --lr 0.01 --batch_size 16 --num_epochs 20
ml-train --fold 0 --lr 0.01 --batch_size 32 --num_epochs 20

# Compare in TensorBoard
tensorboard --logdir runs/
```

---

## Getting Help

### Check Documentation

- [Installation Guide](installation.md) - Setup issues
- [Data Preparation](data-preparation.md) - Dataset organization
- [Configuration Reference](../configuration/) - All parameters explained
- [Troubleshooting](../reference/troubleshooting.md) - Common issues
- [FAQ](../reference/faq.md) - Frequently asked questions

### Verify System Info

```bash
python -c "
import sys, torch, torchvision
print(f'Python: {sys.version}')
print(f'PyTorch: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
"
```

---

## Summary

**You've learned how to:**
- ✅ Run your first training with example dataset
- ✅ Organize custom dataset and generate CV splits
- ✅ Monitor training with TensorBoard and logs
- ✅ Adjust hyperparameters via CLI
- ✅ Resume interrupted training
- ✅ Run inference on trained models
- ✅ Navigate output structure

**Ready for more?** Check out the [full training guide](../user-guides/training.md) for advanced workflows.

---

## Quick Reference Card

```bash
# === SETUP (one time per dataset) ===
ml-split --raw_data data/my_dataset/raw --output data/my_dataset/splits --folds 5
ml-init-config data/my_dataset

# === FIND OPTIMAL LR (OPTIONAL) ===
ml-lr-finder --config configs/my_dataset_config.yaml
ml-lr-finder --config configs/my_dataset_config.yaml --start_lr 1e-7 --end_lr 1  # Custom range

# === TRAINING ===
ml-train --config configs/my_dataset_config.yaml --fold 0                    # Default
ml-train --config configs/my_dataset_config.yaml --fold 0 --num_epochs 50    # Custom epochs
ml-train --config configs/my_dataset_config.yaml --fold 0 --batch_size 32 --lr 0.01  # Custom params

# === CROSS-VALIDATION ===
for fold in {0..4}; do ml-train --config configs/my_dataset_config.yaml --fold $fold; done

# === INFERENCE ===
ml-inference --checkpoint_path runs/hymenoptera_base_fold_0/weights/best.pt

# === EXPORT ===
ml-export --checkpoint runs/hymenoptera_base_fold_0/weights/best.pt --validate

# === MONITORING ===
tensorboard --logdir runs/
tail -f runs/hymenoptera_base_fold_0/logs/train.log
nvidia-smi
```

**Happy training!**
