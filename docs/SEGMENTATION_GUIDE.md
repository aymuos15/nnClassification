# Semantic Segmentation Guide

This guide covers the semantic segmentation capabilities added to the framework, including dataset preparation, model training, and evaluation.

## Table of Contents

1. [Overview](#overview)
2. [Quick Start](#quick-start)
3. [Dataset Preparation](#dataset-preparation)
4. [Configuration](#configuration)
5. [Training](#training)
6. [Evaluation & Metrics](#evaluation--metrics)
7. [Available Models & Losses](#available-models--losses)
8. [Advanced Topics](#advanced-topics)
9. [Troubleshooting](#troubleshooting)

## Overview

**What is Semantic Segmentation?**

Semantic segmentation is a computer vision task that assigns a class label to every pixel in an image. Unlike classification (which predicts a single label for the entire image), segmentation produces a pixel-level prediction map.

**Key Characteristics:**
- **Input**: RGB image (H×W×3)
- **Output**: Label map (H×W) where each pixel value = class index
- **Use Cases**: Medical imaging, autonomous driving, satellite imagery, object boundary detection

**Framework Support:**

The framework provides full support for semantic segmentation with:
- ✅ Task-agnostic pipeline (automatically detects segmentation from config)
- ✅ Specialized datasets (SegmentationDataset) for image-mask pairs
- ✅ Segmentation-specific losses (Dice, IoU, BCE)
- ✅ Segmentation metrics (IoU, Dice coefficient, pixel accuracy)
- ✅ U-Net architecture (Simple U-Net with 4 encoder/decoder levels)
- ✅ All trainer types supported (standard, mixed precision, accelerate, DP, federated)

## Quick Start

### End-to-End Segmentation Workflow

```bash
# 1. Prepare your dataset
# Structure:
# data/my_segmentation/
# ├── images/
# │   ├── img1.jpg
# │   └── img2.jpg
# └── masks/
#     ├── img1.png  # Single-channel PNG with class indices
#     └── img2.png

# 2. Create cross-validation splits
ml-split --raw_data data/my_segmentation/images \
         --mask_data data/my_segmentation/masks \
         --folds 5 \
         --task segmentation

# 3. Generate configuration
ml-init-config data/my_segmentation --task segmentation

# 4. Train the model
ml-train --config configs/my_segmentation_config.yaml

# 5. Run inference
ml-inference --checkpoint_path runs/my_segmentation_fold_0/weights/best.pt
```

That's it! The framework handles all task-specific logic automatically.

## Dataset Preparation

### Directory Structure

**Required structure:**
```
data/{dataset_name}/
├── images/                 # Original RGB images
│   ├── img1.jpg
│   ├── img2.jpg
│   └── ...
├── masks/                  # Segmentation masks (matching filenames)
│   ├── img1.png
│   ├── img2.png
│   └── ...
└── splits/                 # Generated by ml-split
    ├── test_images.txt
    ├── test_masks.txt
    ├── fold_0_train_images.txt
    ├── fold_0_train_masks.txt
    ├── fold_0_val_images.txt
    ├── fold_0_val_masks.txt
    └── ...
```

### Mask Format Requirements

**Critical**: Masks must follow this format:

1. **File format**: PNG or TIFF (lossless formats only, NO JPEG!)
2. **Channels**: Single-channel (grayscale) image
3. **Data type**: 8-bit or 16-bit unsigned integer
4. **Pixel values**: Class indices (0, 1, 2, ..., num_classes-1)
   - 0 = background (or first class)
   - 1 = class 1
   - 2 = class 2
   - etc.
5. **Dimensions**: Same width and height as corresponding image
6. **Filenames**: Must match image filenames (e.g., `img1.jpg` → `img1.png`)

**Example mask creation (Python):**

```python
import numpy as np
from PIL import Image

# Create a 3-class segmentation mask (256x256)
# Class 0 (background), Class 1 (object 1), Class 2 (object 2)
mask = np.zeros((256, 256), dtype=np.uint8)
mask[50:150, 50:150] = 1  # Object 1
mask[150:200, 150:200] = 2  # Object 2

# Save as PNG
Image.fromarray(mask).save('mask.png')
```

**Common mistakes to avoid:**
- ❌ Using RGB masks (convert to single-channel)
- ❌ Using JPEG format (introduces compression artifacts)
- ❌ Using float values (use integer class indices)
- ❌ Using 255 for class 1 (use actual class index: 0, 1, 2, ...)
- ❌ Mismatched dimensions between image and mask
- ❌ Different filenames between image and mask

### Creating Splits

```bash
# Basic segmentation split
ml-split --raw_data data/my_dataset/images \
         --mask_data data/my_dataset/masks \
         --folds 5 \
         --task segmentation

# With custom test/validation splits
ml-split --raw_data data/my_dataset/images \
         --mask_data data/my_dataset/masks \
         --folds 5 \
         --task segmentation \
         --test_split 0.2 \
         --val_split 0.15

# With stratification (ensures balanced class distribution)
ml-split --raw_data data/my_dataset/images \
         --mask_data data/my_dataset/masks \
         --folds 5 \
         --task segmentation \
         --stratified
```

**Output:** Creates paired index files for images and masks in `data/my_dataset/splits/`.

## Configuration

### Generating Segmentation Config

```bash
# Auto-generate with segmentation defaults
ml-init-config data/my_dataset --task segmentation

# With custom settings
ml-init-config data/my_dataset \
  --task segmentation \
  --batch_size 8 \
  --num_epochs 100 \
  --lr 0.001
```

### Key Configuration Settings

**Minimal segmentation config:**

```yaml
data:
  task: 'segmentation'
  data_dir: 'data/my_dataset'
  num_classes: 3  # Including background class

model:
  type: 'custom'
  custom_architecture: 'simple_unet'
  in_channels: 3
  num_classes: 3

loss:
  type: 'dice_loss'

training:
  batch_size: 8  # Segmentation is memory-intensive
  num_epochs: 100
  trainer_type: 'mixed_precision'  # Recommended for GPU

optimizer:
  type: 'adam'
  lr: 0.001
```

**Advanced config with all options:**

```yaml
data:
  task: 'segmentation'
  data_dir: 'data/medical_scans'
  num_classes: 4  # background + 3 organ classes
  fold: 0

model:
  type: 'custom'
  custom_architecture: 'simple_unet'
  in_channels: 3
  num_classes: 4
  dropout: 0.1  # Optional dropout in decoder

loss:
  type: 'dice_bce_loss'  # Combined Dice + BCE
  dice_bce:
    dice_weight: 0.7
    bce_weight: 0.3

training:
  batch_size: 4  # Reduce if OOM errors
  num_epochs: 150
  trainer_type: 'mixed_precision'
  amp_dtype: 'float16'

  # Callbacks
  callbacks:
    - type: 'early_stopping'
      monitor: 'val_iou'
      patience: 20
      mode: 'max'

    - type: 'model_checkpoint'
      monitor: 'val_dice'
      save_top_k: 3
      mode: 'max'

optimizer:
  type: 'adamw'
  lr: 0.0001
  weight_decay: 0.01

scheduler:
  type: 'cosine'
  T_max: 150
  eta_min: 0.00001

transforms:
  train:
    - type: 'resize'
      size: [512, 512]
    - type: 'random_horizontal_flip'
      p: 0.5
    - type: 'random_vertical_flip'
      p: 0.5
    - type: 'normalize'
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

  val:
    - type: 'resize'
      size: [512, 512]
    - type: 'normalize'
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
```

## Training

### Basic Training

```bash
# Train with config
ml-train --config configs/my_segmentation_config.yaml

# Train specific fold
ml-train --config configs/my_segmentation_config.yaml --fold 2

# Override batch size
ml-train --config configs/my_segmentation_config.yaml --batch_size 4
```

### Training with Different Trainers

**Standard (CPU or single GPU):**
```yaml
training:
  trainer_type: 'standard'
```

**Mixed Precision (recommended for GPU):**
```yaml
training:
  trainer_type: 'mixed_precision'
  amp_dtype: 'float16'  # or 'bfloat16' for newer GPUs
```

**Multi-GPU:**
```bash
# One-time setup
accelerate config

# Train with accelerate
training:
  trainer_type: 'accelerate'
```

**Differential Privacy (for sensitive medical data):**
```yaml
training:
  trainer_type: 'dp'
  batch_size: 16
  dp:
    noise_multiplier: 1.1
    max_grad_norm: 1.0
    target_epsilon: 3.0
    target_delta: 1e-5
```

### Memory Optimization

Segmentation models are memory-intensive. If you encounter OOM (Out of Memory) errors:

1. **Reduce batch size**: Start with `batch_size: 4` or even `2`
2. **Use mixed precision**: Set `trainer_type: 'mixed_precision'`
3. **Reduce image size**: Use smaller input dimensions in transforms
4. **Use gradient accumulation**:
   ```yaml
   training:
     batch_size: 2
     gradient_accumulation_steps: 4  # Effective batch size = 2×4 = 8
   ```
5. **Simplify model**: Use fewer encoder/decoder levels (requires custom model)

### Monitoring Training

**TensorBoard:**
```bash
tensorboard --logdir runs/my_segmentation_fold_0/tensorboard
```

**Metrics tracked:**
- Loss curves (train/val)
- IoU per class and mean IoU
- Dice coefficient per class and mean Dice
- Pixel accuracy
- Predicted mask visualizations (sampled from validation set)

## Evaluation & Metrics

### Segmentation Metrics

The framework computes three primary metrics:

**1. IoU (Intersection over Union) / Jaccard Index**
- Measures overlap between predicted and ground truth masks
- Range: [0, 1], higher is better
- Formula: IoU = (Prediction ∩ Ground Truth) / (Prediction ∪ Ground Truth)
- Reported per class and as mean IoU (mIoU)

**2. Dice Coefficient / F1 Score**
- Similar to IoU, emphasizes agreement
- Range: [0, 1], higher is better
- Formula: Dice = 2 × (Prediction ∩ Ground Truth) / (|Prediction| + |Ground Truth|)
- Reported per class and as mean Dice

**3. Pixel Accuracy**
- Percentage of correctly classified pixels
- Range: [0, 1], higher is better
- Simple but can be misleading for imbalanced datasets

**Example output:**
```
Test Results:
  Mean IoU: 0.7821
  Mean Dice: 0.8765
  Pixel Accuracy: 0.9234

  Per-class IoU:
    Class 0 (background): 0.9456
    Class 1 (organ_1): 0.7123
    Class 2 (organ_2): 0.6884
```

### Running Inference

**Standard inference:**
```bash
ml-inference --checkpoint_path runs/my_seg_fold_0/weights/best.pt
```

**With Test-Time Augmentation (TTA):**
```bash
ml-inference --checkpoint_path runs/my_seg_fold_0/weights/best.pt --tta
```

**Ensemble from multiple folds:**
```bash
ml-inference --ensemble \
  runs/my_seg_fold_0/weights/best.pt \
  runs/my_seg_fold_1/weights/best.pt \
  runs/my_seg_fold_2/weights/best.pt
```

### Interpreting Results

**Good segmentation performance:**
- Mean IoU > 0.7 (general guideline)
- Balanced per-class metrics (no single class dominates)
- Visual inspection shows clean boundaries

**Signs of issues:**
- Large gap between train and val metrics → overfitting
- One class has very low IoU → class imbalance or poor masks
- High pixel accuracy but low IoU → model predicts mostly background

## Available Models & Losses

### Models

**Simple U-Net** (built-in):
```yaml
model:
  type: 'custom'
  custom_architecture: 'simple_unet'
  in_channels: 3
  num_classes: N
  dropout: 0.1  # Optional
```

Architecture:
- 4 encoder levels (downsampling)
- 4 decoder levels (upsampling)
- Skip connections between encoder and decoder
- Final 1×1 conv for class prediction

**Adding custom models:**

See `ml_src/core/network/custom.py` and register your architecture:

```python
class MyCustomUNet(nn.Module):
    def __init__(self, in_channels=3, num_classes=2, **kwargs):
        super().__init__()
        # Your architecture here

    def forward(self, x):
        # Returns (batch, num_classes, H, W)
        return output

# Register
MODEL_REGISTRY['my_custom_unet'] = MyCustomUNet
```

### Loss Functions

**Dice Loss** (default for segmentation):
```yaml
loss:
  type: 'dice_loss'
```
- Good for imbalanced datasets
- Optimizes Dice coefficient directly
- Smooth and differentiable

**Dice + BCE Loss** (recommended for most cases):
```yaml
loss:
  type: 'dice_bce_loss'
  dice_bce:
    dice_weight: 0.7
    bce_weight: 0.3
```
- Combines Dice and Binary Cross-Entropy
- Balances region overlap and pixel-wise accuracy
- More stable training than Dice alone

**IoU Loss**:
```yaml
loss:
  type: 'iou_loss'
```
- Optimizes IoU directly
- Can be less stable than Dice

**Adding custom losses:**

See `ml_src/core/losses/segmentation.py` and register your loss:

```python
class MyCustomLoss(nn.Module):
    def __init__(self, config):
        super().__init__()

    def forward(self, logits, targets):
        # logits: (N, C, H, W)
        # targets: (N, H, W)
        return loss_value

# Register
LOSS_REGISTRY['my_custom_loss'] = MyCustomLoss
```

## Advanced Topics

### Class Imbalance

If one class (e.g., background) dominates:

**1. Use Dice Loss** (already handles imbalance well):
```yaml
loss:
  type: 'dice_loss'
```

**2. Focal Loss** (for extreme imbalance):
```python
# Add to ml_src/core/losses/segmentation.py
class FocalLoss(nn.Module):
    def __init__(self, config, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, logits, targets):
        ce_loss = F.cross_entropy(logits, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()
```

**3. Class weights** (weight rare classes higher):
```python
# In config or code
class_weights = torch.tensor([0.1, 1.0, 2.0])  # background, class1, class2
criterion = nn.CrossEntropyLoss(weight=class_weights)
```

### Multi-Scale Training

Train on multiple resolutions for better generalization:

```yaml
transforms:
  train:
    - type: 'random_resize_crop'
      sizes: [[256, 256], [384, 384], [512, 512]]
      p: 0.5
```

### Data Augmentation Best Practices

**Recommended augmentations for segmentation:**
- ✅ Horizontal/vertical flips (anatomy is symmetric)
- ✅ Rotations (90°, 180°, 270° for medical images)
- ✅ Elastic deformations (for medical imaging)
- ✅ Brightness/contrast (but be conservative)
- ❌ Avoid cropping (unless you resize back to original size)
- ❌ Avoid color jitter (can distort medical images)

**Example:**
```yaml
transforms:
  train:
    - type: 'resize'
      size: [512, 512]
    - type: 'random_horizontal_flip'
      p: 0.5
    - type: 'random_rotation'
      degrees: 15
    - type: 'color_jitter'
      brightness: 0.2
      contrast: 0.2
```

### Post-Processing

Common post-processing techniques (apply after inference):

**1. CRF (Conditional Random Field):**
Refines boundaries using image information.

**2. Morphological operations:**
Remove small noise, fill holes.

```python
from scipy import ndimage

# Remove small objects
pred_mask = ndimage.binary_opening(pred_mask)

# Fill holes
pred_mask = ndimage.binary_closing(pred_mask)
```

**3. Connected component analysis:**
Keep only largest connected region.

## Troubleshooting

### Common Issues

**1. OOM (Out of Memory) Errors**
```
RuntimeError: CUDA out of memory
```
**Solution:**
- Reduce batch size to 2 or 1
- Use mixed precision training
- Reduce input image size
- Use gradient accumulation

**2. Poor Segmentation Performance (Low IoU)**

**Possible causes:**
- Incorrect mask format (check pixel values are class indices, not RGB)
- Class imbalance (background dominates) → use Dice loss
- Insufficient training epochs → increase epochs or reduce learning rate
- Overfitting → add dropout, augmentation, or regularization

**3. Loss is NaN**

**Causes:**
- Learning rate too high → reduce by 10x
- Gradient explosion → enable gradient clipping:
  ```yaml
  training:
    callbacks:
      - type: 'gradient_clipping'
        max_norm: 1.0
  ```

**4. Masks Not Loading Correctly**

**Check:**
```python
from PIL import Image
import numpy as np

mask = Image.open('data/my_dataset/masks/img1.png')
mask_array = np.array(mask)

print(f"Shape: {mask_array.shape}")  # Should be (H, W), not (H, W, 3)
print(f"Unique values: {np.unique(mask_array)}")  # Should be [0, 1, 2, ...], not [0, 255]
print(f"Dtype: {mask_array.dtype}")  # Should be uint8 or uint16
```

**5. Mismatched Image and Mask Dimensions**

Ensure images and masks have identical dimensions:
```python
from PIL import Image

img = Image.open('data/my_dataset/images/img1.jpg')
mask = Image.open('data/my_dataset/masks/img1.png')

print(f"Image size: {img.size}")  # (W, H)
print(f"Mask size: {mask.size}")  # Should match image
```

### Performance Tips

**1. Use mixed precision training:**
```yaml
training:
  trainer_type: 'mixed_precision'
  amp_dtype: 'float16'
```
→ 2-3x speedup with minimal accuracy loss

**2. Optimize data loading:**
```yaml
training:
  num_workers: 4  # Adjust based on CPU cores
  pin_memory: true
```

**3. Use smaller architectures for fast iteration:**
Start with Simple U-Net (4 levels), then scale up if needed.

**4. Profile your training:**
```python
import torch.profiler as profiler

# In training loop
with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA]):
    # Training step
    pass
```

## Examples

### Medical Imaging Example

```bash
# Dataset: Brain MRI segmentation (3 classes: background, tumor, edema)
# Images: 256x256 grayscale MRI scans
# Masks: 256x256 single-channel PNG (values 0, 1, 2)

# 1. Split data
ml-split --raw_data data/brain_mri/images \
         --mask_data data/brain_mri/masks \
         --folds 5 \
         --task segmentation \
         --stratified

# 2. Generate config
ml-init-config data/brain_mri --task segmentation

# 3. Edit config for medical imaging
# config changes:
#   - num_classes: 3
#   - loss: dice_bce_loss
#   - trainer_type: mixed_precision
#   - batch_size: 8

# 4. Train
ml-train --config configs/brain_mri_config.yaml

# 5. Evaluate with ensemble
ml-inference --ensemble \
  runs/brain_mri_fold_0/weights/best.pt \
  runs/brain_mri_fold_1/weights/best.pt \
  runs/brain_mri_fold_2/weights/best.pt \
  runs/brain_mri_fold_3/weights/best.pt \
  runs/brain_mri_fold_4/weights/best.pt
```

### Satellite Imagery Example

```bash
# Dataset: Land cover segmentation (5 classes)
# Images: 512x512 RGB satellite images
# Masks: 512x512 single-channel PNG (values 0-4)

# 1. Split data
ml-split --raw_data data/landcover/images \
         --mask_data data/landcover/masks \
         --folds 3 \
         --task segmentation

# 2. Generate config
ml-init-config data/landcover --task segmentation

# 3. Train with larger image size
ml-train --config configs/landcover_config.yaml \
         --batch_size 4 \
         --loss dice_loss

# 4. Export for deployment
ml-export --checkpoint runs/landcover_fold_0/weights/best.pt \
          --validate --benchmark
```

## Next Steps

- **Extend models**: Add custom U-Net variants to `ml_src/core/network/custom.py`
- **Add losses**: Implement task-specific losses in `ml_src/core/losses/segmentation.py`
- **Optimize augmentations**: Tune transforms in config for your domain
- **Try federated learning**: Distribute training across hospitals/sites
- **Deploy models**: Export to ONNX and integrate with production pipelines

For more details, see:
- [Configuration Reference](../configuration/README.md)
- [Training Guide](../user-guides/training.md)
- [Development Guide](../development/README.md)
