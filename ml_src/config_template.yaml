# Base configuration for PyTorch classifier training

# Reproducibility configuration
seed: 42
deterministic: false  # Set to true for full determinism (slower but completely reproducible)

# Data configuration
data:
  # Dataset name (used in run directory naming)
  dataset_name: 'hymenoptera'

  # Base directory: must contain raw/ (images) and splits/ (index files from splitting.py)
  data_dir: 'data/hymenoptera_data'

  # Cross-validation settings
  fold: 0                     # Which fold to use (0-indexed)

  num_workers: 4

# Training configuration
training:
  # Trainer type: Selects the training strategy
  # Options: 'standard', 'mixed_precision', 'accelerate', 'dp'
  # - 'standard': Traditional PyTorch training (default, backward compatible)
  # - 'mixed_precision': Automatic Mixed Precision (AMP) training for faster training with reduced memory
  # - 'accelerate': HuggingFace Accelerate for multi-GPU/distributed training
  # - 'dp': Differential privacy training with Opacus (requires: pip install opacus)
  # Can be omitted - defaults to 'standard'
  trainer_type: 'standard'

  # Mixed precision settings (only used when trainer_type: 'mixed_precision')
  # amp_dtype: 'float16'  # Options: 'float16' (faster, default) or 'bfloat16' (more stable)
  # Note: Mixed precision requires CUDA. Falls back to standard training on CPU.

  # Accelerate settings (only used when trainer_type: 'accelerate')
  # gradient_accumulation_steps: 1  # Accumulate gradients over N batches (useful for large models)
  # Usage:
  #   Single device: python ml-train --config config.yaml
  #   Multi-GPU: accelerate launch ml-train --config config.yaml
  # Note: Multi-GPU requires one-time setup with `accelerate config`

  # Differential privacy settings (only used when trainer_type: 'dp')
  # Requires installation: pip install opacus OR pip install -e ".[dp]"
  # dp:
  #   noise_multiplier: 1.1  # Privacy noise level (higher = more privacy, lower utility)
  #   max_grad_norm: 1.0     # Gradient clipping threshold (lower = more privacy)
  #   target_epsilon: 3.0    # Privacy budget target (lower = stronger privacy, < 10 recommended)
  #   target_delta: 1e-5     # Privacy delta (typically 1/dataset_size)
  # Privacy-Utility Tradeoff:
  #   - Epsilon < 1: Very strong privacy (high utility loss)
  #   - Epsilon 1-10: Good privacy-utility balance
  #   - Epsilon > 10: Weaker privacy guarantees
  # Note: Training with DP is slower and may require more epochs for convergence

  batch_size: 4
  num_epochs: 3
  device: 'cuda:0'  # Will fallback to cpu if cuda not available

  # Model EMA (Exponential Moving Average) configuration
  # EMA maintains a shadow copy of model weights updated as a moving average during training.
  # Often improves test accuracy by 0.5-2% with no additional training cost.
  # Recommended for production deployments and competitive benchmarks.
  ema:
    enabled: false           # Enable EMA (default: false)
    decay: 0.9999            # Decay rate for EMA updates (0.999-0.9999 typical)
                             # Higher decay = slower update, smoother weights
                             # Common values: 0.999 (fast), 0.9999 (standard), 0.99999 (slow)
    warmup_steps: 0          # Skip EMA updates for first N training steps (default: 0)
                             # Useful to let model stabilize before tracking EMA
                             # Typical: 0 (no warmup) or 2000-5000 (with warmup)

  # Early stopping configuration
  early_stopping:
    enabled: false         # Enable early stopping to prevent overfitting
    patience: 10           # Number of epochs to wait for improvement before stopping
    metric: 'val_acc'      # Metric to monitor: 'val_acc' or 'val_loss'
    mode: 'max'            # 'max' for accuracy (higher is better), 'min' for loss (lower is better)
    min_delta: 0.0         # Minimum change in metric to qualify as improvement

# Inference settings
inference:
  # Inference strategy: Selects the inference optimization strategy
  # Options: 'standard', 'mixed_precision', 'accelerate', 'tta', 'ensemble', 'tta_ensemble'
  # - 'standard': Traditional PyTorch inference (default, backward compatible, works on CPU/GPU)
  # - 'mixed_precision': Automatic Mixed Precision (AMP) inference for 2-3x speedup on GPU
  # - 'accelerate': HuggingFace Accelerate for multi-GPU/distributed inference
  # - 'tta': Test-Time Augmentation for improved robustness (slower, higher accuracy)
  # - 'ensemble': Combine multiple model checkpoints (e.g., from different CV folds)
  # - 'tta_ensemble': Combined TTA + Ensemble for maximum performance (slowest, best accuracy)
  # Can be omitted - defaults to 'standard'
  strategy: 'standard'

  # Mixed precision inference settings (only used when strategy: 'mixed_precision')
  # amp_dtype: 'float16'  # Options: 'float16' (faster, default) or 'bfloat16' (more stable)
  # Note: Mixed precision requires CUDA. Falls back to standard inference on CPU.

  # ============================================================================
  # TEST-TIME AUGMENTATION (TTA) SETTINGS
  # ============================================================================
  # Used when strategy: 'tta' or 'tta_ensemble'
  # Applies multiple augmented versions of each test image and aggregates predictions
  # for improved robustness. Slower but can improve accuracy by 1-3%.
  #
  # tta:
  #   # Augmentations to apply during inference
  #   # Options: 'horizontal_flip', 'vertical_flip', 'rotate_90', 'rotate_180', 'rotate_270'
  #   augmentations:
  #     - 'horizontal_flip'
  #     - 'vertical_flip'
  #
  #   # How to aggregate predictions across augmentations
  #   # Options:
  #   #   - 'mean': Average logits (soft voting, recommended for best accuracy)
  #   #   - 'max': Take maximum logits
  #   #   - 'voting': Hard voting (majority vote on predicted classes)
  #   aggregation: 'mean'

  # ============================================================================
  # ENSEMBLE SETTINGS
  # ============================================================================
  # Used when strategy: 'ensemble' or 'tta_ensemble'
  # Combines predictions from multiple models trained on different folds or runs
  # for improved generalization. Requires multiple checkpoints.
  #
  # ensemble:
  #   # List of checkpoint paths to ensemble
  #   checkpoints:
  #     - 'runs/my_dataset_fold_0/weights/best.pt'
  #     - 'runs/my_dataset_fold_1/weights/best.pt'
  #     - 'runs/my_dataset_fold_2/weights/best.pt'
  #     - 'runs/my_dataset_fold_3/weights/best.pt'
  #     - 'runs/my_dataset_fold_4/weights/best.pt'
  #
  #   # How to aggregate predictions across models
  #   # Options:
  #   #   - 'soft_voting': Average logits (recommended for best accuracy)
  #   #   - 'hard_voting': Majority vote on predicted classes
  #   #   - 'weighted': Weighted average using weights below
  #   aggregation: 'soft_voting'
  #
  #   # Optional: Weights for each model (only used with aggregation: 'weighted')
  #   # Must match number of checkpoints. If omitted, uses equal weights.
  #   # weights: [0.25, 0.20, 0.20, 0.20, 0.15]  # Sum should be 1.0 (auto-normalized)

# Optimizer configuration
optimizer:
  lr: 0.001
  momentum: 0.9

# Learning rate scheduler configuration
scheduler:
  step_size: 7
  gamma: 0.1

# Model configuration
model:
  # Model type: 'base' for torchvision models, 'custom' for custom architectures
  type: 'base'

  # For base models: specify any torchvision architecture
  # Options: resnet18, resnet50, vgg16, efficientnet_b0, mobilenet_v2, etc.
  architecture: 'resnet18'

  # For custom models: specify custom architecture name
  # Options: 'simple_cnn', 'tiny_net'
  custom_architecture: null

  # Number of output classes
  num_classes: 2

  # Pretrained weights: 'DEFAULT' for ImageNet weights, null for random initialization
  weights: null

  # Additional parameters for custom models
  input_size: 224  # Input image size (for custom models)
  dropout: 0.5     # Dropout probability (for custom models that support it)

  # Legacy option (deprecated but kept for compatibility)
  model_path: 'best_model.pth'

# Data augmentation configuration
transforms:
  train:
    resize: [224, 224]
    random_horizontal_flip: true
    normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

  val:
    resize: [224, 224]
    normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

  test:
    resize: [224, 224]
    normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

# ============================================================================
# HYPERPARAMETER SEARCH (OPTIONAL)
# ============================================================================
# Uncomment and configure this section to enable hyperparameter optimization
# Requirements:
#   pip install -e ".[optuna]"
# Or generate a config with search pre-configured:
#   ml-init-config data/my_dataset --optuna
#
# Usage:
#   ml-search --config config.yaml --n-trials 50
#   ml-visualise --mode search --study-name my_study
#
# search:
#   # Study configuration
#   study_name: 'hymenoptera_optimization'
#   storage: 'sqlite:///optuna_studies.db'  # Or postgresql://user:pass@host/db
#   n_trials: 50
#   timeout: null  # Optional: stop after N seconds
#   
#   # Optimization settings
#   direction: 'maximize'  # 'maximize' for accuracy, 'minimize' for loss
#   metric: 'val_acc'      # Metric to optimize: 'val_acc' or 'val_loss'
#   
#   # Sampler configuration
#   sampler:
#     type: 'TPESampler'          # Options: TPESampler, RandomSampler, GridSampler, CmaEsSampler
#     n_startup_trials: 10        # Random trials before TPE optimization
#     
#   # Pruner: Early stopping of unpromising trials
#   pruner:
#     type: 'MedianPruner'        # Options: MedianPruner, PercentilePruner, HyperbandPruner
#     n_startup_trials: 5         # Don't prune first N trials
#     n_warmup_steps: 5           # Don't prune first N epochs within trial
#     
#   # Cross-validation settings
#   cross_validation:
#     enabled: false              # Enable CV for robust hyperparameter selection
#     n_folds: 5                  # Number of CV folds to use
#     aggregation: 'mean'         # How to aggregate: 'mean', 'median', 'best', 'worst'
#     
#   # Search space definitions
#   search_space:
#     # Learning rate (log scale between 1e-5 and 1e-1)
#     optimizer.lr:
#       type: 'loguniform'
#       low: 1e-5
#       high: 1e-1
#       
#     # Batch size (discrete choices)
#     training.batch_size:
#       type: 'categorical'
#       choices: [16, 32, 64]
#       
#     # Optimizer momentum (uniform between 0.8 and 0.99)
#     optimizer.momentum:
#       type: 'uniform'
#       low: 0.8
#       high: 0.99
#       
#     # Weight decay (log scale)
#     optimizer.weight_decay:
#       type: 'loguniform'
#       low: 1e-6
#       high: 1e-2
#       
#     # Scheduler step size (integer between 5 and 15)
#     scheduler.step_size:
#       type: 'int'
#       low: 5
#       high: 15
#       
#     # Scheduler gamma
#     scheduler.gamma:
#       type: 'uniform'
#       low: 0.05
#       high: 0.5
#       
#     # Model dropout
#     model.dropout:
#       type: 'uniform'
#       low: 0.1
#       high: 0.7
#   
#   # Advanced: Architecture search (uncomment to enable)
#   # search_space:
#   #   model.architecture:
#   #     type: 'categorical'
#   #     choices: ['resnet18', 'resnet34', 'efficientnet_b0']
